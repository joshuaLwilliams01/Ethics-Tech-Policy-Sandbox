{
  "level": 3,
  "title": "AI, Child Safety & Burden of Responsibility",
  "scenarios": [
    {
      "scenario_id": "L3-S1",
      "title": "Hash or Hesitate?",
      "prompt": "A social media platform must choose between a fast AI method on users' devices to quickly detect child sexual abuse material (CSAM), which raises privacy concerns, and a slower server method that protects privacy but delays removal of harmful content. What would be your approach?",
      "choices": {
        "A": "Perform scans directly on user devices using checks from reliable sources, ensuring all requests are encrypted to maintain privacy and security.",
        "B": "Implement server-side scanning with takedown measures, create educational resources, and share information on harmful content.",
        "C": "Customize scanning protocols based on environments: some allow user device scans, while others require server scans."
      },
      "toolkit_cues": "How do we balance child safety with privacy by default? What mix limits harm?",
      "p3_cues": "People (victim safety), Planet (scale inference), Parity (over-policing certain groups).",
      "toolkit_flow": {
        "order": [],
        "prompts": [],
        "quick_actions": {
          "A": [
            "Establish clear false positive appeal process",
            "Create educational resources on reporting",
            "Develop privacy-preserving detection methods"
          ],
          "B": [
            "Establish clear false positive appeal process",
            "Create educational resources on reporting",
            "Develop privacy-preserving detection methods"
          ],
          "C": [
            "Establish clear false positive appeal process",
            "Create educational resources on reporting",
            "Develop privacy-preserving detection methods"
          ]
        },
        "metrics": [
          "Detection accuracy rate",
          "False positive rate",
          "Appeal success rate"
        ],
        "owner_required": true,
        "review_default_days": 90
      },
      "toolkit_references": "Values Explainer Cards, Weighing Options"
    },
    {
      "scenario_id": "L3-S2",
      "title": "Safer Generative AI (Gen-AI)",
      "prompt": "Recent findings indicate that a creative generative AI tool can be manipulated into producing inappropriate content involving minors when subjected to adversarial prompts. What course of action would you recommend?",
      "choices": {
        "A": "Implement safety protocols, use watermarking for transparency, and create fast appeal processes for flagged content.",
        "B": "Refine datasets to exclude harmful material, slow release pace, and conduct red-teaming before public deployment.",
        "C": "Restrict access to potentially sensitive features, allowing only verified researchers to utilize them for responsible experimentation."
      },
      "toolkit_cues": "What abuse paths and secondary-trauma risks exist? What gates and response plans are required?",
      "p3_cues": "People (reviewer well-being), Planet (retraining cycles), Parity (access for small orgs).",
      "toolkit_flow": {
        "order": [],
        "prompts": [],
        "quick_actions": {
          "A": [
            "Conduct comprehensive red-teaming exercises",
            "Establish clear safety protocols and watermarking",
            "Create appeal process for false flags"
          ],
          "B": [
            "Conduct comprehensive red-teaming exercises",
            "Establish clear safety protocols and watermarking",
            "Create appeal process for false flags"
          ],
          "C": [
            "Conduct comprehensive red-teaming exercises",
            "Establish clear safety protocols and watermarking",
            "Create appeal process for false flags"
          ]
        },
        "metrics": [
          "Safety protocol compliance %",
          "False positive rate",
          "Red-team findings"
        ],
        "owner_required": true,
        "review_default_days": 60
      },
      "toolkit_references": "Future Story, Ethics Frame"
    },
    {
      "scenario_id": "L3-S3",
      "title": "Reports at Scale",
      "prompt": "Law enforcement has requested the ability to submit bulk uploads to the platform's abuse Application Programming Interface (API). However, this may lead to an increase in false reports. What would be your recommended approach?",
      "choices": {
        "A": "Implement bulk uploads but require human review and establish rate limits to prevent abuse.",
        "B": "Prioritize matches that have high confidence levels and establish strong processes for handling appeals.",
        "C": "Direct submissions through an independent clearinghouse to verify reports before they are processed."
      },
      "toolkit_cues": "Do faster reports outweigh wrongful flags? Who reviews, how fast, and with what limits?",
      "p3_cues": "People (mislabeling harm), Planet (processing load), Parity (small platform capacity).",
      "toolkit_flow": {
        "order": [],
        "prompts": [],
        "quick_actions": {
          "A": [
            "Establish rate limits and abuse prevention measures",
            "Create independent verification process",
            "Develop strong appeal process for false reports"
          ],
          "B": [
            "Establish rate limits and abuse prevention measures",
            "Create independent verification process",
            "Develop strong appeal process for false reports"
          ],
          "C": [
            "Establish rate limits and abuse prevention measures",
            "Create independent verification process",
            "Develop strong appeal process for false reports"
          ]
        },
        "metrics": [
          "False report rate",
          "Appeal success rate",
          "Processing time"
        ],
        "owner_required": true,
        "review_default_days": 90
      },
      "toolkit_references": "Impact Explorer, Ethics Frame"
    },
    {
      "scenario_id": "L3-S4",
      "title": "Schools use AI",
      "prompt": "Several middle schools are using AI to monitor students to prevent violence. A recent investigation has uncovered potential security risks associated with this practice.",
      "choices": {
        "A": "Advocate for stronger regulations on AI use in schools to prioritize student privacy and security.",
        "B": "Encourage schools to be transparent about AI monitoring practices and engage in open conversations with parents and students.",
        "C": "Propose alternative measures for ensuring student safety that do not involve AI monitoring, like enhanced conflict resolution programs and better mental health support."
      },
      "toolkit_cues": "Is monitoring the least intrusive means for safety? What non-AI measures could replace it?",
      "p3_cues": "People (student trust), Planet (data storage), Parity (guardian consent gaps).",
      "toolkit_flow": {
        "order": [],
        "prompts": [],
        "quick_actions": {
          "A": [
            "Conduct security audit of AI monitoring systems",
            "Establish clear consent process for parents and students",
            "Develop alternative safety measures"
          ],
          "B": [
            "Conduct security audit of AI monitoring systems",
            "Establish clear consent process for parents and students",
            "Develop alternative safety measures"
          ],
          "C": [
            "Conduct security audit of AI monitoring systems",
            "Establish clear consent process for parents and students",
            "Develop alternative safety measures"
          ]
        },
        "metrics": [
          "Security incident rate",
          "Consent rate %",
          "Alternative program effectiveness"
        ],
        "owner_required": true,
        "review_default_days": 60
      },
      "toolkit_references": "Values Explainer Cards, Weighing Options"
    },
    {
      "scenario_id": "L3-S5",
      "title": "Survivor-First Takedown",
      "prompt": "Victims of online harassment seek quick removal of harmful content, while some agencies may need copies for investigations, balancing swift action with the need to preserve evidence. What would your decision be?",
      "choices": {
        "A": "Implement a rapid removal process, retaining only hashed proof for verification.",
        "B": "Conduct a gradual removal process, ensuring consent from survivors and utilizing evidence escrow for accountability.",
        "C": "Establish a court-mandated expedited removal route, with a standard procedure for all other cases."
      },
      "toolkit_cues": "How does speed vs. evidence retention affect survivors? What safe proof and appeal paths exist?",
      "p3_cues": "People (re-trauma risk), Planet (storage), Parity (legal access pathways).",
      "toolkit_flow": {
        "order": [],
        "prompts": [],
        "quick_actions": {
          "A": [
            "Establish survivor consent process",
            "Create evidence escrow system",
            "Develop expedited removal protocols"
          ],
          "B": [
            "Establish survivor consent process",
            "Create evidence escrow system",
            "Develop expedited removal protocols"
          ],
          "C": [
            "Establish survivor consent process",
            "Create evidence escrow system",
            "Develop expedited removal protocols"
          ]
        },
        "metrics": [
          "Removal time",
          "Survivor satisfaction",
          "Investigation success rate"
        ],
        "owner_required": true,
        "review_default_days": 30
      },
      "toolkit_references": "Impact Explorer, Ethics Frame"
    }
  ]
}
